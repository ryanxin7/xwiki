---
author: Ryan
title: 2.Prometheus 应用监控
date: 2024-03-13
tags: [Prometheus]
sidebar_position: 3
---




## 一、添加监控一个普通应用（内置metrics接口）
对于普通应用只需要能够提供一个满足 prometheus 格式要求的 /metrics 接口就可以让 Prometheus 来接管监控，比如 Kubernetes 集群中非常重要的 **CoreDNS** 插件，一般默认情况下就开启了 /metrics 接口：

```yaml
kubectl get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  creationTimestamp: "2024-03-12T03:16:32Z"
  name: coredns
  namespace: kube-system
  resourceVersion: "233"
  uid: 0a83d2a3-4f9f-4c16-b4a6-ce0bd41d711d
```

### 查看Cluster IP 与端口
上面 ConfigMap 中 `prometheus :9153` 就是开启 prometheus 的插件：
```bash
$ kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
NAME                       READY   STATUS    RESTARTS      AGE   IP           NODE       NOMINATED NODE   READINESS GATES
coredns-7d89d9b6b8-6d9nk   1/1     Running   2 (56s ago)   26h   10.244.0.4   master01   <none>           <none>
coredns-7d89d9b6b8-wt74f   1/1     Running   2 (56s ago)   26h   10.244.0.5   master01   <none>           <none>
```

可以先尝试手动访问下 /metrics 接口，如果能够手动访问到那证明接口是没有任何问题的：
```bash
$ curl http://10.244.0.4:9153/metrics
# HELP coredns_build_info A metric with a constant '1' value labeled by version, revision, and goversion from which CoreDNS was built.
# TYPE coredns_build_info gauge
coredns_build_info{goversion="go1.16.4",revision="053c4d5",version="1.8.4"} 1
# HELP coredns_cache_entries The number of elements in the cache.
# TYPE coredns_cache_entries gauge
coredns_cache_entries{server="dns://:53",type="denial"} 1
coredns_cache_entries{server="dns://:53",type="success"} 0
# HELP coredns_cache_misses_total The count of cache misses.
# TYPE coredns_cache_misses_total counter
coredns_cache_misses_total{server="dns://:53"} 1
# HELP coredns_dns_request_duration_seconds Histogram of the time (in seconds) each request took per zone.
# TYPE coredns_dns_request_duration_seconds histogram
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.00025"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.0005"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.001"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.002"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.004"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.008"} 0
coredns_dns_request_duration_seconds_bucket{server="dns://:53",zone=".",le="0.016"} 0
......
```


### 添加静态收集目标
可以看到可以正常访问到，从这里可以看到 CoreDNS 的监控数据接口是正常的了，然后我们就可以将这个 /metrics 接口配置到 `prometheus.yml` 中去了，直接加到默认的 prometheus 这个 job 下面：

```yaml
# prometheus-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitor
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s

    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']

    - job_name: 'coredns'
      static_configs:
      - targets: ['10.244.0.4:9153', '10.244.0.5:9153']
```

当然，我们这里只是一个很简单的配置，`scrape_configs` 下面可以支持很多参数，例如：

- `basic_auth` 和 `bearer_token`：比如我们提供的 /metrics 接口需要 basic 认证的时候，通过传统的用户名/密码或者在请求的 header 中添加对应的 token 都可以支持
- `kubernetes_sd_configs` 或 `consul_sd_configs`：可以用来自动发现一些应用的监控数据。

---

> `kubernetes_sd_configs`是用于在 Kubernetes 环境中进行服务发现的配置选项。
> `consul_sd_configs`是用于在 Consul 中进行服务发现的配置选项。



### 更新Prometheus ConfigMap 资源对象
现在我们重新更新这个 ConfigMap 资源对象：
```bash
$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
```


### **热更新**
  现在 Prometheus 的配置文件内容已经更改了，隔一会儿被挂载到 Pod 中的 `prometheus.yml` 文件也会更新，由于我们之前的 Prometheus 启动参数中添加了 `--web.enable-lifecycle` 参数，所以现在我们只需要执行一个 reload 命令即可让配置生效：

```bash
$ kubectl get pods -n monitor -o wide
NAME                         READY   STATUS    RESTARTS        AGE   IP           NODE     NOMINATED NODE   READINESS GATES
prometheus-cc4945f67-9xrx8   1/1     Running   1 (4m21s ago)   22h   10.244.1.6   node01   <none>           <none>
$ curl -X POST "http://10.244.1.6:9090/-/reload"
```


由于 ConfigMap 通过 Volume 的形式挂载到 Pod 中去的热更新需要一定的间隔时间才会生效，所以需要稍微等一小会儿。<br />这个时候我们再去看 Prometheus 的 Dashboard 中查看采集的目标数据：<br />


![coredns](http://img.xinn.cc/coredns.png)


可以看到我们刚刚添加的 coredns 这个任务已经出现了，然后同样的我们可以切换到 Graph 下面去，我们可以找到一些 CoreDNS 的指标数据，至于这些指标数据代表什么意义，一般情况下，我们可以去查看对应的 /metrics 接口，里面一般情况下都会有对应的注释。<br />


metrics explorer
![metrics-explorer](http://img.xinn.cc/metrics-explorer.png)


<br />到这里我们就在 Prometheus 上配置了第一个 Kubernetes 应用。


## 二、使用 Exporter 监控
  上面我们也说过有一些应用可能没有自带 /metrics 接口供 Prometheus 使用，在这种情况下，我们就需要利用 exporter 服务来为 Prometheus 提供指标数据了。

Prometheus 官方为许多应用就提供了对应的 exporter 应用，也有许多第三方的实现，我们可以前往官方网站进行查看：[exporters](https://prometheus.io/docs/instrumenting/exporters/)，当然如果你的应用本身也没有 exporter 实现，那么就要我们自己想办法去实现一个 /metrics 接口了，只要你能提供一个合法的 /metrics 接口，Prometheus 就可以监控你的应用。


### 以 **sidecar 的形式部署**[**redis-exporter**](https://github.com/oliver006/redis_exporter)** 服务**
   比如我们这里通过一个 [**redis-exporter**](https://github.com/oliver006/redis_exporter) 的服务来监控 redis 服务，对于这类应用，我们一般会以 **sidecar 的形式和主应用部署在同一个 Pod 中**，比如我们这里来部署一个 redis 应用，并用 redis-exporter 的方式来采集监控数据供 Prometheus 使用，如下资源清单文件：
```yaml
# prometheus-redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: monitor
spec:
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:4
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
            - containerPort: 6379
        - name: redis-exporter
          image: oliver006/redis_exporter:latest
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
            - containerPort: 9121
---
kind: Service
apiVersion: v1
metadata:
  name: redis
  namespace: monitor
spec:
  selector:
    app: redis
  ports:
    - name: redis
      port: 6379
      targetPort: 6379
    - name: prom
      port: 9121
      targetPort: 9121
```


可以看到上面我们在 redis 这个 Pod 中包含了两个容器，一个就是 redis 本身的主应用，另外一个容器就是 **redis_exporter**。现在直接创建上面的应用：

```bash
$ kubectl apply -f prometheus-redis.yaml
deployment.apps/redis created
service/redis created
```

创建完成后，我们可以看到 redis 的 Pod 里面包含有两个容器：

```bash
$ kubectl get pods -n monitor
NAME                         READY   STATUS    RESTARTS      AGE
prometheus-cc4945f67-9xrx8   1/1     Running   1 (10m ago)   22h
redis-7fb8ff6779-bpwfj       2/2     Running   0             95s

$  kubectl get svc -n monitor
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
prometheus   NodePort    10.108.88.125    <none>        9090:30090/TCP      22h
redis        ClusterIP   10.110.110.210   <none>        6379/TCP,9121/TCP   112s
```

### 检查metrics端口
我们可以通过 9121 端口来校验是否能够采集到数据：

```bash
$ curl 10.110.110.210:9121/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 7
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.17.5"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 730376
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 730376
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 4218
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter

```

### 添加 Prometheus targets 配置
同样的，现在我们只需要更新 Prometheus 的配置文件：
```yaml
- job_name: "redis"
  static_configs:
    - targets: ["redis:9121"]
```

由于我们这里是通过 Service 去配置的 redis 服务，当然直接配置 Pod IP 也是可以的，因为和 Prometheus 处于同一个 namespace，所以我们直接使用 servicename 即可。配置文件更新后，重新加载：

### 热更新
```bash
$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.1.6:9090/-/reload"
```


### 查看采集目标数据
这个时候我们再去看 Prometheus 的 Dashboard 中查看采集的目标数据：


Dashboard-redis
![Dashboard-redis](http://img.xinn.cc/Dashboard-redis.png)


可以看到配置的 redis 这个 job 已经生效了。切换到 Graph 下面可以看到很多关于 redis 的指标数据，我们选择任意一个指标，比如 `redis_exporter_scrapes_total`，然后点击执行就可以看到对应的数据图表了：<br />



![redis_exporter_scrapes_total](http://img.xinn.cc/redis_exporter_scrapes_total.png)




## 三、集群节点监控介绍
前面我们学习了怎样用 Promethues 来监控 Kubernetes 集群中的应用，但是对于 Kubernetes 集群本身的监控也是非常重要的，我们需要时时刻刻了解集群的运行状态。<br />对于集群的监控一般我们需要考虑以下几个方面：

- **Kubernetes 节点的监控**：比如节点的 cpu、load、disk、memory 等指标
- **内部系统组件的状态**：比如 kube-scheduler、kube-controller-manager、kubedns/coredns 等组件的详细运行状态
- **编排级的 metrics**：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标


**Kubernetes 集群的监控方案目前主要有以下几种方案：**

- **Heapster**：Heapster 是一个集群范围的监控和数据聚合工具，以 Pod 的形式运行在集群中。 heapster 除了 Kubelet/cAdvisor 之外，我们还可以向 Heapster 添加其他指标源数据，比如 kube-state-metrics，需要注意的是 Heapster 已经被废弃了，后续版本中会使用 metrics-server 代替。
- **cAdvisor**：[cAdvisor](https://github.com/google/cadvisor) 是 Google 开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器。
- **kube-state-metrics**：[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) 通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。
- **metrics-server**：metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。

**不过 kube-state-metrics 和 metrics-server 之间还是有很大不同的，二者的主要区别如下**：

- kube-state-metrics 主要关注的是业务相关的一些元数据，比如 Deployment、Pod、副本状态等
- metrics-server 主要关注的是[资源度量 API](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md) 的实现，比如 CPU、文件描述符、内存、请求延时等指标。


### 通过 [node_exporter](https://github.com/prometheus/node_exporter) 采集节点指标 
**node_exporter **就是抓取用于采集服务器节点的各种运行指标，目前 **node_exporter** 支持几乎所有常见的监控点，比如 conntrack，cpu，diskstats，filesystem，loadavg，meminfo，netstat 等，详细的监控点列表可以参考其 [Github 仓库](https://github.com/prometheus/node_exporter)。

#### 通过 DaemonSet 部署 node-exporter
我们可以通过 DaemonSet 控制器来部署该服务，这样每一个节点都会自动运行一个这样的 Pod，如果我们从集群中删除或者添加节点后，也会进行自动扩展。<br />在部署 **node-exporter** 的时候有一些细节需要注意，如下资源清单文件：

```yaml
# prometheus-node-exporter.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitor
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostPID: true
      hostIPC: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: node-exporter
          image: prom/node-exporter:v1.3.1
          args:
            - --web.listen-address=$(HOSTIP):9100
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --no-collector.hwmon # 禁用不需要的一些采集器
            - --no-collector.nfs
            - --no-collector.nfsd
            - --no-collector.nvme
            - --no-collector.dmi
            - --no-collector.arp
            - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/containerd/.+|/var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)
            - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
          ports:
            - containerPort: 9100
          env:
            - name: HOSTIP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
          resources:
            requests:
              cpu: 150m
              memory: 180Mi
            limits:
              cpu: 150m
              memory: 180Mi
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          volumeMounts:
            - name: proc
              mountPath: /host/proc
            - name: sys
              mountPath: /host/sys
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      tolerations:
        - operator: "Exists"
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: dev
          hostPath:
            path: /dev
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /

```

#### 定义监控指标数据变量
由于我们要获取到的数据是主机的监控指标数据，而我们的 **node-exporter **是运行在容器中的，所以我们在 Pod 中需要配置一些 Pod 的安全策略，这里我们就添加了 `hostPID: true`、`hostIPC: true`、`hostNetwork: true` 3 个策略，用来使用主机的 `**PID namespace**`、`**IPC namespace**` 以及主机网络，这些 namespace 就是用于容器隔离的关键技术，要注意这里的 namespace 和集群中的 namespace 是两个完全不相同的概念。<br />另外我们还将主机的 `/dev`、`/proc`、`/sys`这些目录挂载到容器中，这些因为我们采集的很多节点数据都是通过这些文件夹下面的文件来获取到的，比如我们在使用 top 命令可以查看当前 cpu 使用情况，数据就来源于文件 `/proc/stat`，使用 free 命令可以查看当前内存使用情况，其数据来源是来自 `/proc/meminfo` 文件。<br />另外由于我们集群使用的是 kubeadm 搭建的，所以如果希望 master 节点也一起被监控，则需要添加相应的容忍，然后直接创建上面的资源对象：

```bash
$ kubectl apply -f prometheus-node-exporter.yaml
daemonset.apps/node-exporter created

$ kubectl get pods -n monitor  -l app=node-exporter -o wide
NAME                  READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
node-exporter-6558t   1/1     Running   0          56s   10.0.0.204   node02     <none>           <none>
node-exporter-8khfw   1/1     Running   0          56s   10.0.0.203   node01     <none>           <none>
node-exporter-xkh6f   1/1     Running   0          56s   10.0.0.202   master01   <none>           <none>
```
部署完成后，我们可以看到在几个节点上都运行了一个 Pod，由于我们指定了` hostNetwork=true`，所以在每个节点上就会绑定一个端口 9100，我们可以通过这个端口去获取到监控指标数据：

#### 测试metrics接口

```bash
$ curl 10.0.0.203:9100/metrics
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
```
当然如果你觉得上面的手动安装方式比较麻烦，我们也可以使用 Helm 的方式来安装：<br />`helm upgrade --install node-exporter --namespace monitor stable/prometheus-node-exporter`


### 服务自动发现
  由于我们这里每个节点上面都运行了 **node-exporter** 程序，如果我们**通过一个 Service 来将数据收集到一起用静态配置的方式配置到 Prometheus 去中，就只会显示一条数据**，我们得自己在指标数据中去过滤每个节点的数据，当然我们也可以手动的把所有节点用静态的方式配置到 Prometheus 中去，但是以后要**新增或者去掉节点的时候就还得手动去配置**，那么有没有一种方式可以让 Prometheus 去自动发现我们节点的 node-exporter 程序，并且按节点进行分组呢？这就是 Prometheus 里面非常重要的服务发现功能了。

#### 服务发现模式
在 Kubernetes 下，Promethues 通过与 Kubernetes API 集成，主要支持 5 种服务发现模式，分别是：**Node、Service、Pod、Endpoints、Ingress**。


#### Node 服务发现模式
我们通过 kubectl 命令可以很方便的获取到当前集群中的所有节点信息：
```bash
$ kubectl get nodes
NAME      STATUS   ROLES                  AGE   VERSION
master1   Ready    control-plane,master   11d   v1.22.8
node1     Ready    <none>                 11d   v1.22.8
node2     Ready    <none>                 11d   v1.22.8
```
但是要让 Prometheus 也能够获取到当前集群中的所有节点信息的话，我们就需要利用 Node 的服务发现模式，同样的，在 `prometheus.yml` 文件中配置如下的 job 任务即可：

```yaml
- job_name: "nodes"
  kubernetes_sd_configs:
    - role: node
```

通过指定 `kubernetes_sd_configs` 的模式为 `node`，Prometheus 就会自动从 Kubernetes 中发现所有的 node 节点并作为当前 job 监控的目标实例，发现的节点 /metrics 接口是默认的 kubelet 的 HTTP 接口。<br />prometheus 的 ConfigMap 更新完成后，同样的我们执行 reload 操作，让配置生效：

```bash
$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.1.6:9090/-/reload"
```


#### 替换exporter端口

配置生效后，我们再去 prometheus 的 dashboard 中查看 Targets 是否能够正常抓取数据。<br />


![exporter-Bad-Request2](http://img.xinn.cc/exporter-Bad-Request2.png)


<br />我们可以看到上面的 nodes 这个 job 任务已经自动发现了我们 3 个 node 节点，但是在获取数据的时候失败了，出现了类似于下面的错误信息：

```bash
server returned HTTP status 400 Bad Request
```

这个是因为 prometheus 去发现 Node 模式的服务的时候，**访问的端口默认是 10250，而默认是需要认证的 https 协议才有权访问的，但实际上我们并不是希望让去访问 10250 端口的 /metrics 接口，而是 node-exporter 绑定到节点的 9100 端口**，所以我们应该将这里的 10250 替换成 9100，但是应该怎样替换呢？



这里我们就需要使用到 Prometheus 提供的 `relabel_configs` 中的` replace` 能力了，`relabel` 可以在 Prometheus **采集数据之前**，通过 Target 实例的 Metadata 信息，动态重新写入 Label 的值。除此之外，我们还能根据 Target 实例的 Metadata 信息选择是否采集或者忽略该 Target 实例。<br />比如我们这里就可以去匹配` __address__ `这个 Label 标签，然后替换掉其中的端口，如果你不知道有哪些 Label 标签可以操作的话，可以在 Service Discovery 页面获取到相关的元标签，这些标签都是我们可以进行 Relabel 的标签：



![Relabel](http://img.xinn.cc/Relabel.png)


<br />现在我们来替换掉端口，修改 ConfigMap：

```yaml
- job_name: "nodes"
  kubernetes_sd_configs:
    - role: node
  relabel_configs:
    - source_labels: [__address__]
      regex: "(.*):10250"
      replacement: "${1}:9100"
      target_label: __address__
      action: replace
```
这里就是一个正则表达式，去匹配` __address__ `这个标签，然后将 host 部分保留下来，port 替换成了 9100，现在我们重新更新配置文件，执行 reload 操作，然后再去看 Prometheus 的 Dashboard 的 Targets 路径下面 kubernetes-nodes 这个 job 任务是否正常了：




![kubernetes-nodes-Targets](http://img.xinn.cc/kubernetes-nodes-Targets.png)



我们可以看到现在已经正常了，但是还有一个问题就是我们采集的指标数据 Label 标签就只有一个节点的 hostname，这对于我们在进行监控分组分类查询的时候带来了很多不方便的地方，要是我们能够将集群中 Node 节点的 Label 标签也能获取到就很好了。这里我们可以通过 `labelmap` 这个属性来将 Kubernetes 的 Label 标签添加为 Prometheus 的指标数据的标签：

```yaml
- job_name: "kubernetes-nodes"
  kubernetes_sd_configs:
    - role: node
  relabel_configs:
    - source_labels: [__address__]
      regex: "(.*):10250"
      replacement: "${1}:9100"
      target_label: __address__
      action: replace
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)
```
添加了一个 `action` 为 `labelmap`，正则表达式是` __meta_kubernetes_node_label_(.+)` 的配置，这里的意思就是表达式中匹配都的数据也添加到指标数据的 Label 标签中去。<br />对于 `kubernetes_sd_configs` 下面可用的元信息标签如下：

- `**__meta_kubernetes_node_name**`：节点对象的名称
- `**_meta_kubernetes_node_label**`：节点对象中的每个标签
- `**_meta_kubernetes_node_annotation**`：来自节点对象的每个注释
- `**_meta_kubernetes_node_address**`：每个节点地址类型的第一个地址（如果存在）

关于 kubernets_sd_configs 更多信息可以查看官方文档：[kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Ckubernetes_sd_config%3E)


#### 监控kubelet

kubelet https证书<br />另外由于 kubelet 也自带了一些监控指标数据，就上面我们提到的 10250 端口，所以我们这里也把 kubelet 的监控任务也一并配置上：

```yaml
- job_name: "kubelet"
  kubernetes_sd_configs:
    - role: node
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)
```

但是这里需要特别注意的是这里必须使用 https 协议访问，这样就必然需要提供证书，我们这里是通过配置 `insecure_skip_verify: true` 来跳过了证书校验，但是除此之外，要访问集群的资源，还必须要有对应的权限才可以，也就是对应的 ServiceAccount 绑定权限允许才可以，我们这里部署的 prometheus 关联的 ServiceAccount 对象前面我们已经提到过了，这里我们只需要将 Pod 中自动注入的 `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt `和 `/var/run/secrets/kubernetes.io/serviceaccount/token` 文件配置上，就可以获取到对应的权限了。

现在我们再去更新下配置文件，执行 reload 操作，让配置生效，然后访问 Prometheus 的 Dashboard 查看 Targets 路径：<br />

reload-node-Targets

![reload-node-Targets](http://img.xinn.cc/reload-node-Targets.png)


现在可以看到我们上面添加的 **kubernetes-kubelet** 和 **kubernetes-nodes** 这两个 job 任务都已经配置成功了，而且二者的 Labels 标签都和集群的 node 节点标签保持一致了。

现在我们就可以切换到 Graph 路径下面查看采集的一些指标数据了，比如查询 `node_load1` 指标：<br />


![node_load1](http://img.xinn.cc/node_load1.png)


我们可以看到将几个节点对应的 **node_load1** 指标数据都查询出来了，同样的，我们还可以使用 PromQL 语句来进行更复杂的一些聚合查询操作，还可以根据我们的 Labels 标签对指标数据进行聚合，比如我们这里只查询 node1 节点的数据，可以使用表达式 `node_load1{instance="node02"}` 来进行查询：


![instance-node02](http://img.xinn.cc/instance-node02.png)


到这里我们就把 Kubernetes 集群节点使用 Prometheus 监控起来了，接下来我们再来和大家学习下怎样监控 Pod 或者 Service 之类的资源对象。




