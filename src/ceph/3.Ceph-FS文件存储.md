---
author: Ryan
title: Ceph 集群维护 （三）
date: 2023-01-11
lastmod: 2023-05-31
tags: 
    - 分布式存储
categories: 
   - Ceph
expirationReminder:
  enable: true
---


https://docs.ceph.com/en/latest/cephfs/<br />[链接](https://docs.ceph.com/en/latest/cephfs/)


> Ceph FS即**Ceph Filesy Stem**,可以实现文件系统共享功能,客户端通过ceph协议挂载并使用ceph集群作为数据存储服务器。


Ceph FS在公司中使用常场景相对比较多，主要用于动静分离，多服务数据共享例如Nginx 。

Ceph被多个服务同时挂载，写入数据时能实时同步，类似NFS。

![image.png](https://cdn1.ryanxin.live/xxlog/1669602704359-0c6e3198-b50f-40e2-89dd-d5174582a165.png)

**客户端通过ceph协议挂载**<br />Linux内核版本>2.6.34 就内置Cpeh模块无需安装

**MDS存储池用于存储Ceph FS上存储的文件相关的元数据**<br />Ceph FS需要运行Meta Data Services(MDS)服务，其守护进程为**ceph-mds**, ceph-mds进程管理与cephFS上存储的文件相关的元数据，并协调对ceph存储集群的访问。

**mate data pool**：用于存储Ceph FS上存储的文件相关的元数据，pool名称可以随意指定。<br />**ceph data pool **：用来保存客户端上传到Ceph的数据。

![image.png](https://cdn1.ryanxin.live/xxlog/1669603166367-c946e2e0-1a32-43b3-b0df-124d5d008899.png)


## 3.1 部署MDS服务
在指定的ceph-mds服务器,部署ceph-mds服务，可以和其它服务器混用(如ceph-mon.<br />ceph-mgr)

**Ubuntu:**
```bash
#查看当前可用版本
root@ceph-mgr1:~# sudo su - root
root@ceph-mgr1:~#  apt-cache madison ceph-mds 
#选择版本安装
root@ceph-mgr1:~# apt install ceph-mds=16.2.5-1bionic
```

**CentOS:**
```bash
Centos:
[root@ceph-mgr1 ~]# yum install ceph-mds -y
```

部署：
```bash
[ceph@ceph-deploy ceph-cluster]$ ceph-deploymds create ceph-mgr1

```


## 3.2 验证 MDS 服务
MDS服务目前还无法正常使用，需要为MDS创建存储池用于保存MDS的数据.
```bash
[ceph@ceph-deploy ceph-clusterI$ ceph mds stat
1 up:standby 
#当前为备用状态，需要分配pool才可以使用，
```


## 3.3 创建CephFS metadata和data存储池:
使用CephFS之前需要事先于集群中创建一个文件系统，并为其分别指定**元数据**和**数据相**<br />**关的存储池**，如下命令将创建名为mycephfs的文件系统，它使用**cephfs-metadata**作为<br />元数据存储池，使用**cephfs-data**为数据存储池:


```bash
#保存 metadata的pool
[ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32
pool 'cephfs-metadata' created 

#生产环境下matedata数据的pg数量为16个就可以了（数据量小 10几个T的数量元数据才几个G）

#保存客户端数据的pool
[ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64
pool 'cephfs-data' created 

[ceph@ceph-deploy ceph-cluster]$ ceph -S
#当前ceph状态

#查看当前存储池
ceph@ceph-dep Loy :~/ ceph-cluster$ ceph osd pool ls
device health_ metrics
32
mypool
myrbd1
.rgw.root
default.rgw.Log
default.rgw.control
default.rgw.meta
cephfs-metadata
cepnfs-data
```

## 3.4 创建Ceph FS 并验证

Ceph FS在早期版本中一个集群中只能创建一个，现在支持启用多个
```bash
ceph fs new  fs_name metadata data 

#--allow-dangerous-metadata-overlay:允许非安全的元数据写入

 [ceph@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata cephfs-data

#查看Ceph FS状态
cephaceph-dep Loy:~/ ceph-cluster$ ceph -S
cluster:
id:
5ac860ab- 9a4e- 4edd 9da2 e3de293a8d44
health: HEALTH 0K
se rvices :
mon: 3 daemons, auorum ceph-mon1 , ceph-mon2 , ceph-mon3 (age 48m)
mgr: ceph-mg r1(active, since 47m), standbys: ceph-mgr2
mds: 1/1 daemons up
osd:20 osds: 20 up (since 47m), 20 in(since 6d)
rgw: 1 daemon active (1 hosts, 1 zones )

data :
voLumes: 1/1 healthy 
pools:
10 pools, 329 pgs
objects: 253 objects, 89 MiB
usage : 562 MiB used, 2.0 TiB / 2.0 TiB avail
pgs: 329 act ive+c Lean

io: 
Client : 1.3 KiB/S wr ,0 op/s rd, 4 op/s wr


cephaceph-deploy :~/ceph-cluster$ ceph mds stat
mycephfs:1 {0=ceph-mgr1=up: active}

#active表示启用成功
```

## 3.5 客户端挂载Ceph FS
在ceph的客户端测试CephFS的挂载，需要指定mon节点（负责提供认证）的6789端口: <br />（6789就是CephFS对外提供挂载的端口）

```bash
#使用admin权限挂载
[ceph@ceph-deploy ceph-clusterl$ cat ceph.client.admin.keyring
[client.admin]
   key = AQCrVhZhof2zKxAATltgtgAdDteHSAGFEyE/nw==
   caps mds = "allow *"
   caps mgr = "allow *"
   caps mon = "allow *"
   caps osd = "allow *"
```

**ubuntu及centos client 挂载(内核版本2.6.34在3.6.34及以上)**
```bash
#使用key挂载
#ip写mon集群中的任意一个都行，也可以写三个提供高可用
#挂载到/mnt目录
root@ceph-client3-ubuntu1 804:-# mount -t ceph 172.31.6.101:6789:/ /mnt
-o name=admin,secret=AQCrVhZhof2zKxAATltgtgAdDteHSAGFEyE/mw==

```

查看挂载情况<br />![image.png](https://cdn1.ryanxin.live/xxlog/1669605911936-e6a7f84e-56d9-4da9-ad29-24d706599e3f.png)

在任何一个节点变更数据，会立即在其他客户端同步显示，非常适合多节点的web服务。

### 模拟web多节点服务场景下，数据同步效果
```bash
#安装nginx
yum install epel-release -y
yum install nginx -y
#创建数据目录
mkdir /data/nginx/statics 
#挂载
root@ceph-client3-ubuntu1804:~# mount一t ceph 172.31.6.101:6789:/ /data/nginx/statics -o name=admin ,secret=AQA3dhdhMd/UABAA2SNpJ+hcK1dD5L2Hj5XMg== 

```

```bash
vim /etc/ngxin.conf


server {
    listen       80;
    listen       [::] : 80;
    server_name  _;
    root         /data/nginx/statics;

cd /data/nginx/statics

#上传文件

```


尽管内核中自带CephFS组件，因为性能关系还是建议使用最新的内核模块，安装Ceph ceph common （ceph 的公共组件）

Ubuntu源：
```bash
vim /etc/
deb https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic main

#导入key
wget -q -O- 'https ://download.ceph.com/keys/release.asc' | sudo apt-key add -
apt update

#导入key
wget -q -O- 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc' | sudo
apt-key add -


```

CentOS  安装16版本之前的客户端
```bash
wget https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-15.2.15/el7/x86_64/ceph-common-15.2.15-0.el7.x86_64.rpm
```
### 16版本的CephFS状态
![image.png](https://cdn1.ryanxin.live/xxlog/1669605042090-f48f24ec-d38f-46c7-bf50-d120690729ec.png)

### 13版本的CephFS状态

![image.png](https://cdn1.ryanxin.live/xxlog/1669605075412-881c1d1a-cb39-4b96-b065-85fffd037559.png)


压测工具：<br />Jmeter


## 3.6 命令总结
列出存储池并显示id
```bash
$ ceph osd lspools 
$ ceph osd Ispools
1 device_ health metrics
2 mypool
3 cephfs-metadata
4 cephfs-data

```

查看pg状态

```bash
$ceph pg stat
$ceph pg stat
129 pgs: 129 active+clean; 319 KIB data, 1.1 GiB used, 2.0 TiB / 2.0 TiB avail
```

查看指定pool或所有的pool的状态
```bash
$ ceph osd pool stats mypool 
pool myrdb1 id 2
nothing is going on
```

查看集群存储状态
```bash
$ ceph df detail
```

查看osd的状态
```bash
ceph osd stat
```

显示OSD底层详细信息
```bash
ceph osd dump
```

显示OSD和node节点对于关系
```bash
ceph osd tree
```

查看mon节点状态
```bash
ceph mon stat 
```

查看mon详细信息
```bash
ceph mon dump
```
