---
author: Ryan
title: Ceph RBD （六）
date: 2023-01-14
lastmod: 2023-01-14
tags: 
    - 分布式存储
categories:
   - Ceph
expirationReminder:
  enable: true
---



## 6.1 RBD架构图 

Ceph可以同时提供对象存储**RADOSGW** 、**块存储RBD**、**文件系统存储Ceph FS**<br />RBD即**RADOS Block Device**的简称，RIBD 块存储是常用的存储类型之一，RBD块设备类似磁盘可以被挂载，RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中，<br />**条带化技术**就是一种自动的将1/0 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去.这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的/O并行能力，从而获得非常好的性能。

![image.png](https://cdn1.ryanxin.live/xxlog/1669948660767-92e57330-688f-40eb-b794-bfe6f0629769.png)

## 6.2 创建存储池 
```bash
[ceph@ceph-deploy ceph-cluster]$ ceph osd pool create rbd-data1 32 32
pool 'rbd-data1' created


#在存储池启用rbd
[ceph@ceph-deploy ceph-cluster]$ ceph osd pool appliclition enable rbd-data1 rbd
enabled application 'rbd' on pool 'rbd-data1'

#初始化rbd:
[ceph@ceph-deploy ceph-cluster]$ rbd pool init -P rbd-data1
```


## 6.3 创建img镜像 
rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像(image) ，并把映像文件作为块设备使用。<br />rbd 命令可用于创建、查看及删除块设备相在的映像(image) ，以及克隆映像、创建快照.将映像回滚到快照和查看快照等管理操作。<br />例如，下面的命令能够在指定的RBD即rbd-data1创建一个名为myimg1的映像:

### 6.3.1 创建镜像
创建镜像命令格式:<br /> 
```bash
#创建两个镜像:
$ rbd create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering
$ rbd create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering

#验证镜像:
$ rbd ls --pool rbd-data1
data-img1
data-img2

#列出镜像个多信息:
$ rbd Is --pool rbd-data1 -| 1
NAME       SIZE PARENT FMT PROT LOCK
data-img1  3 GiB        2
data-img2  5 GiB        2
```


### 6.3.2 查看镜像详细信息 
```bash
$ rbd --image data-img2 --pool rbd-data1 info
rbd image 'data-img2':
        size 5 GiB in 1280 objects
        order 22 (4 MiB objects) #对象大小，每个对象是2^22/1024/1024=4MiB
        id: d42b6b8b4567 #镜像id
        block_name_prefix: rbd_data.d42b6b8b4567 #size里面的1280个对象名称前缀
        format:2 #镜像文件格式版本
        features:layering #特性，layering 支持分层快照以写时复制
        op_features:
        flags:
        create timestamp: Mon Dec 14 12:22:27 2020
```


### 6.3.3 以json格式显示镜像信息:
```bash
$ rbd ls --pool rbd-data1 -| --format json --pretty-format
[
    {
        "image": "data-img1",
        "size": 3221225472,
        "format": 2
    },
    {
        "image": "data-img2",
        "size": 5368709120,
        "format": 2
    }
]

```

### 6.3.4 镜像的其他特性
**特性简介**<br />**layering**:支持镜像分层快照特性，用于快照及写时复制，可以对image创建快照并保护，然后从快照克隆出新的image出来，父子image之间采用COW技术，共享对象数据。<br />**striping**:支持条带化v2，类似raid 0 ，只不过在ceph环境中的数据被分散到不同的对象中，可改善顺序读写场景较多情况下的性能。

**exclusive-lock**: 支持独占锁，限制-一个镜像只能被一个客户端 使用.<br />**object-map**: 支持对象映射(依赖**exclusive-lock**),加速数据导人导出及已用空间统计等，此特性开启的时候，会记录image所有对象的一个位图，用以标记对象是否真的存在，在一些场景下可以加速io。

**fast-diff**: 快速计算镜像与快照数据差异对比(依赖object-map).<br />**deep-flatten**: 支持快照扁平化操作，用于快照管理时解决快照依赖关系等。<br />**journaling**: 修改数据是否记录日志，该特性可以通过记录日志并通过日志恢复数据(依赖独占锁)，开启此特性会增加系统磁盘I0使用. 


jewel默认开启的特性包括: **layering/exlcusive lock/object map/fast diff/deep flatten**

```bash
[ceph@ceph-deploy ceph-cluster]$ rbd help feature enable
usage: rbd feature enable [--pool <pool>] [-image <image>]
                             [--journal-splay-width <jourmal-splay-width>]
                             [--jourmal-object- size <journal-object- size>]
                             [--journal-pool <journal-pool>]
                             <image-spec> <features> [<features> ... ]
Enable the specified image feature.

Positional arguments
<image-spec>                  image specification
                              (example: [<pool-name>/<image-name>)
<features>                    image features
                              [exclusive-lock, object-map, fast-diff, journaling]
Optional arguments
-P [--pool] arg    pool name
--image arg        image name
--journal-splay-width arg number of active journal objects
--joumal-object-size arg size of journal objects [4K <= size <= 64M]
--joumnal-pool arg  pool for journal objects
```


### 6.3.6 镜像特性的启用
启用指定存储池中的指定镜像的特性: 
```bash
$ rbd feature enable exclusive-lock --pool rbd-data1 --image data-img1
$ rbd feature enable object-map --pool rbd-data1 --image data-img1
$ rbd feature enable fast-diff --pool rbd-data1 --image data-img1


#验证镜像特性:
$ rbd -imnage data-img1 --pool rbd-data1 info
rbd image 'data-img1':
        size 3 GiB in 768 objects
        order 22 (4 MiB objects)
        id: d45b6b8b4567
        block_name_prefix: rbd_data.d45b6b8b4567
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff
        op_features:
        flags: object map invalid, fast diff invalid
        create_ timestamp: Mon Dec 14 12:35:44 2020
```

### 6.3.7 镜像特性的禁用
禁用指定存储池中指定镜像的特性
```bash
$ rbd feature disable fast-diff --pool rbd-data1 --image data-img1

#验证镜像特性:
$ rbd --image data-img1 --pool rbd-data1 info
            rbd image 'data-img1':
            size 3 GiB in 768 objects
            order 22 (4 MiB objects)
            id: d45b6b8b4567
            block_name_prefix: rbd_data.d45b6b8b4567
            format: 2
            features: layering, exclusive-lock, object-map #少了一个fast-diff 特性
            op_features:
            flags: object map invalid
            create_timestamp: Mon Dec 14 12:35:44 2020
```

## 6.4 配置客户端使用RBD 
在centos客户端挂载RBD，并分别使用admin及普通用户挂载RBD并验证使用.

### 6.4.1 客户端配置yum源 
客户端要想挂载使用ceph RBD，需要安装ceph客户端组件**ceph-common**，但是<br />ceph-common不在cenos的yum仓库，因此需要单独配置yum源。
```bash
#配置yum源:
yum install epel-release
yum install htps://mirrors.aliyun.com/ceph/rpm-octopus/el7/noarch/ceph-release-1-1.el7.noarch.rpm -y
```


### 6.4.2 客户端安装ceph-common:
```bash
yum install ceph-common
```


### 6.4.3  客户端使用普通用户挂载并使用RBD

创建普通账户并授权:
```bash
#创建普通账户
[ceph@ceph-deploy ceph-cluster]$ ceph auth add clientshjie mon 'allow r' osd 'allow rwx pool=rbd-data1'
added key for client.lije

#验证用户信息
[ceph@ceph-deploy ceph-cluster$ ceph auth get client.shijie
exported keyring for client.shijie
[client.shijie]
          key = AQDHE9hfhzPVCRAAIlRuUIkWQW8YXv/JiLizFuA==
          caps mon = "allow r"
          caps osd = "allow rwx pool=rbd-data1"

#创建空的keyring文件
[ceph@ceph-deploy ceph-cluster]$ ceph-authtool --riate-keyring ceph.client.shijie.keyring
creating ceph.clent.shije.keyring

#导出用户keyring
[ceph@ceph-deploy ceph-cluster]$ ceph auth get client.shijie -o ceph.client.shiie.keyring
exported keyring for client.shjjie

#验证指定用户的keyring文件
[ceph@ceph-deploy ceph-cluster]$ cat ceph.client.shiie.keyring
```

拷贝配置文件与普通账户keyring文件到客户端
```bash
scp ceph.cpnf ceph.client.shiie.keyring root@172.31.6.201:/etc/ceph
```


### 6.4.4 在客户端验证权限
```bash
[root@ceph-client2 ~]# cd /etc/ceph/
root@ceph-client2 ceph]# ls
ceph.client.shije.keyring ceph.conf rbdmap
root@ceph-client2 ceph]# ceph --user shijie -s #默认使用admin账户
cluster:
   id:  23b0f9f2 -8db3-477f-99a7 -35a90eaf3dab
   health: HEALTH_ OK

mgr: ceph-mgr1(active), standbys: ceph-mgr2
mds: mycephfs-1/1/1 up {0=ceph-mgr1=up:active}
osd: 12 osds: 12 up, 12 in
rgw.1 daemon active
data:
pools: 9 pools, 256 pgs
objects: 400 objects, 455 MiB
usage: 14 GiB used, 1.2 TiB / 1.2 TiB avail
pgs: 256 active+clean
```


### 6.4.5 映射rbd
使用普通用户权限映射rbd
```bash
#映射rbd
root@ceph-client2 ceph]# rbd --user shijie -p rbd-data1 map data-img2 /dev/rbd0

#验证rbd
root@ceph- client2 ceph]# fdisk - /dev/rbd0

Disk /dev/rbd0: 5368 MB, 5368709120 bytes, 10485760 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes

```
![image.png](https://cdn1.ryanxin.live/xxlog/1669957144116-253e56a8-a026-400c-bc6f-b54aae46f5a7.png)

### 6.4.6 格式化并使用rbd镜像
```bash
[root@ceph-client2 ceph]# mkfs.ext4 /dev/rbd0
[root@ceph-client2 ceph]# mkdir /data
[root@ceph-client2 ceph]# mount /dev/rbd0 /data/
[root@ceph-client2 ceph]# cp /var/log/messages /data/
root@ceph-client2 ceph]# df -TH

#管理端验证镜像状态
[ceph@ceph-deploy ceph-cluster]$ rbd ls -p rbd-data1 -l
NAME  SIZE PARENT FMT PROT LOCK
data-img1 3 GiB    2      excl  #施加锁文件，已经被客户端映射
data-img2 5 GiB    2
```


### 6.4.7 验证ceph内核模块
挂载rbd之后系统内核会自动加载 **libceph.ko** 模块

```bash
#centos 
lsmod | grep ceph 

modinfo libceph


#ubuntu 
```


### 6.4.8 rbd镜像空间拉伸
可以扩展空间，不建议缩小空间
```bash
#当前rbd镜像空间大小
[ceph@ceph-deploy ceph-cluster$ rbd ls -p rbd-data1 -l
NAME  SIZE PARENT FMT PROT LOCK
data-img1 3 GiB     2   excl
data-ima2 5 GiB     2

#rbd镜像空间拉伸命令
[ceph@ceph-deploy ceph-cluster]$ rbd help resize
usage: rbd resize [--pool <pool>] [--image <image>] --size <size>
                     [--allow-shrink] [--no-progress]
                     <image-spec>
#拉伸rbd镜像空间
[ceph@ceph-deploy ceph-cluster]$ rbd resize --pool rbd-data1 --image data-img2 -size 8G
Resizing image: 100% complete..done.
[ceph@ceph-deploy ceph-cluster]$ rbd ls -p rbd-data1 -l
NAME  SIZE PARENT FMT PROT LOCK
data-img1 3 GiB   2        excl
data-img2 8 GiB   2

客户端验证镜像空间:

[root@ceph-client2 ~]# fdisk -l /dev/rbd0
Disk /dev/rbd0: 8589 MB, 8589934592 bytes, 16777216 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes/ 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes

```

### 6.4.9 开机自动挂载
```bash
[root@ceph-client2 ~]# cat /etc/rc.d/rc.local

ripod --user shijie -p rbd-data1 map data-img1
mount /dev/rbd0 /data/

[root@ceph-client2 ~]# chmod a+x /etc/rc.d/rc.local
[root@ceph-client2 ~]# reboot

#查看映射
[root@ceph-client2 ~]# rbd showmapped
id pool image  snap device
0 rbd-data1 data-img2 -   /dev/rbd0

#验证挂载
[root@ceph-client2 ~]# df -TH
Filesystem    Type   Size Used Avail Use% Mounted on
devtmpfs     devtmpfs 942M  0    942M    0% /dev

```

### 6.4.10 卸载rbd镜像
```bash
[root@ceph-client2 ceph]# umount /data
root@ceph-client2 ceph]# rbd --user shijie -p rbd-data1 unmap data-img2
```

### 6.4.11 删除rbd镜像 
镜像删除后数据也会被删除而且是无法恢复，因此在执行删除操作的时候要慎重。
```bash
[ceph@ceph-deploy ceph-cluster]$ rbd help rm
usage: rbd rm [--pool <pool>] [--image <image>] [--no-progress]
              <image-spec>
```

### 6.4.12 rbd镜像回收站机制
删除的镜像数据无法恢复,但是还有另外一种方法可以先把镜像移动到回收站，后期确认删除的时候再从回收站删除即可。
```bash
[ceph@ceph-deploy ceph-cluster$ rbd help trash 
status       Show the status of this image.
trash list (trash ls)  List trash images.
trash move (trash mv)  Move an image to the trash.
trash purge  Remove all expired images from trash.
trash remove (trash rm)  Remove an image from trash.
trash restore  Restore an image from trash.

#查看镜像状态:
[ceph@ceph-deploy ceph-cluster]$ rbd status --pool rbd-data1 --image data-img2
Watchers:
       watcher=172.31.6.1 10:0/2342274731  client.54552
cookie=18446462598732840961
       watcher=1 72.31.6.111:0/2165319040 client.54558
cookie=18446462598732840961

#将镜像移动到回收站:
[ceph@ceph-deploy ceph-cluster]$ rbd trash move --pool rbd-data1 --image data-img2

#查看回收站的镜像:
[ceph@ceph-deploy ceph-cluster$ rbd trash list --pool rbd-data1
d42b6b8b4567 data-img2


#从回收站删除镜像
如果镜像不再使用，可以直接使用trash remove将其从回收站删除

#还原镜像
[ceph@ceph-deploy ceph-cluster]$lybd trash restore --pool rbd-data1 --image data-img2 --image-id d42b6b8b4567

#验证镜像:
[ceph@ceph-deploy ceph-cluster]$ rbd Is --pool rbd-data1 -1
NAME SIZE PARENT FMT PROT LOCK
data-img2 8 GiB  2
```

## 6.5 镜像快照
```bash
[ceph@ceph-deploy ceph-cluster]$ rbd help snap
snap create (snap add) #创建快照
snap limit clear  #清除镜像的快照数量限制
snap limit set #设置一个镜像的快照上限
snap list (snap ls) #列出快照
snap protect #保护快 照被删除
snap purge #删除所有未保护的快照
snap remove (snap rm)  #删除一个快照
snap rename  #重命名快照
snap rollback (snap revert) #还原快照
snap unprotect  #允许一个快照被删除(取消快照保护)
```

### 6.5.1 创建快照

```bash
(ceph@ceph-deploy ceph-cluster]$ rbd help snap create
usage: rbd snap create [--pool <poob>] [--image <image>] [--snap <snap>]
                          <snap-spec>
#创建快照
$ rbd snap create --pool rbd-data1 --image data-img2 - snap
img2-snap-20201215

#验证快照
$ rbd snap list --pool rbd-data1 --image data-img2
SNAPID NAME        SIZE TIMESTAMP
    4 img2-snap-20201215 8 GiB Tue Dec 15 15:26:20 2020

```

### 6.5.2 删除数据并还原快照
```bash
关闭服务>取消挂载 > 取消映射 

#卸载rbd
root@ceph-client2 ~]# umount /data
root@ceph-client2 ~]# rbd unmap /dev/rbd0

#回滚命令:
[ceph@ceph-deploy ceph-cluster]$ rbd help snap rollback
usage: rbd snap rollback [--pool <pool>] [-image <image>] [--snap <snap>]
                         [--no-progress]
                         <snap-spec>
                         

#回滚快照
[ceph@ceph-deploy ceph-clusterI$ rbd snap rollbaci --pool rbd-data1 --image data-img2 --snap img2-snap-20201215
Rolling back to snapshot: 100% complete..done.

```
### 6.5.3 删除快照
```bash
#删除指定快照
[ceph@ceph-deploy ceph-cluster]$I rbd snap remove --pool rbd-data1 --image data-img2 --snap img2-snap-20201215
Removing snap: 100% complet...done.

#验证快照是否删除
[ceph@ceph-deploy ceph-cluster]$ rbd snap list --pool rbd-data1 --image data-img2



```

### 6.5.4 快照数量限制 
```bash
#设置与修改快照数量限制
[ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 30
[ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 20
[ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 15
```

